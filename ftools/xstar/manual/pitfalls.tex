\chapter{Problems and Pitfalls}
\label{sec:pitfalls}

XSTAR has been designed to be as `user-friendly' as possible while 
still maintaining a large amount of flexibility.  However, experience has 
shown that it is difficult to guard against the many possible misuses of 
the code, and that it is impossible to generate a code which is completely 
free from errors or unintended features.  In this chapter we list
what we consider to be some of the most probable pitfalls 
of the use of XSTAR.  This chapter should be read by any user who runs
XSTAR2XSPEC or who ventures very far beyond the default parameter values 
of XSTAR itself.


\subsection{Nickel and Iron-peak Eelements}

The atomic rates for nickel are less accurate, and less well debugged, than those 
for other elements.  It is recommended that any model be run first with the nickel
abundance set to zero.  Substantial differences between models with zero and non-zero nickel, 
in terms of temperature, opacity, etc., should be treated with great caution.

Similar comments apply to elements introduced in version 2.2:  Li, Be, B, odd-Z elements between 
F and K, plus the iron-peak elements (except for Fe itself).  Many of the rates 
for these elements are scaled hydrogenic, most ions with 3 or more electrons 
have a level structure which is assumed hydrogenic.  Quantitative results affected 
by these elements should be regarded as reliable only for highly ionized models, which 
are dominated by H- or He-like ions.  

\subsection{Mtables}

If you are using mtables with variable abundances, then you will likely get 
totally unphysical results unless you read the section `Important Notes on Mtables'
in the xstar2xspec chapter.

\section{Execution Time}

XSTAR is designed to strike a balance between accuracy and speed,
but this inevitably involves some disparity between different 
computing platforms.  As a result, many problems of interest 
require large amounts of time on machines which are
relatively slow, or which are heavily used for other tasks.  

In an effort to avoid wasted time (and CPU cycles) we offer the 
following suggestions: (i) XSTAR does not attempt to calculate 
ionization, excitation, etc. for elements whose abundances are 
specified to be less than 10$^{-15}$ relative to hydrogen.  
Large reductions in computation time can be achieved by 
zeroing the abundances of elements which are likely to be 
unimportant anyway: calcium, argon, and nickel.  
(ii) For some purposes constant temperature is an adequate
approximation, and is often a useful preliminary step 
in deciding parameter values such as column density, 
and  require a fraction of the execution time of 
full thermal equilibrium models. 
(iii) For some purposes a low column density ($\leq$ 10$^{18}$ cm$^{-2}$)
will provide sufficient information.  Large column densities require
significantly more execution time.  If large columns are needed, then 
execution can be speeded up by use of a large value of emult, or 
a small value of taumax, ans by setting npass to 1. 
(iv) New in version 2.2 is the ability to specify fewer continuum bins 
on input.  Execution time is approximately proportional to the number 
of continuum bins.

The parameter files included in the source tree for both xstar 
and xstar2xspec are set to perform constant temperature models, 
in order to allow the user to become familiar with xstar without 
requiring large investments in computer time.


\section{Low Density}

Although all rates should extrapolate correctly at low density, the 
level population calculation requires the inversion of large matrices 
of rates.  In most cases, the largest elements of the matrix are the 
spontaneous decay rates (A values) for allowed transitions.  For highly 
charged ions these can exceed  10$^{13}$ s$^{-1}$.  At low densities the 
smallest rates of interest are the collisional rates and recombination 
rates.  XSTAR attempts to avoid inverting singular matrices by discarding 
rows and columns whose largest elements differ from the largest element in 
the matrix by more than the machine precision (and then assuming the 
populations in the associated levels are zero).  This can have the effect of 
producing inaccurate solutions particularly at low densities 
since some physically important transitions (notably recombination) 
may be discarded.  Unfortunately, there is no clear way of automatically 
informing the user when this is happening.  A rough rule of thumb is that
densities less than approximately 1000 should be avoided when 
iron may be ionized beyond Fe XVII.  

An indication of possible numerical problems is given by the final 
integer on each line of the output log file.  This is the number of 
iterations required in order to reach thermal equilibrium and charge 
neutrality.  Models with good convergence will typically have values 
of 5 or less for this quantity, except for the first step and possibly 
near ionization fronts.  Otherwise, if this integer is large 
(i.e. greater than, say, 20), and if the value of heating - cooling 
(`h-c') is greater than
1 -- 2$\%$, then it is possible that the density is lower than can be 
treated accurately by XSTAR.



\section{High Density}

All level populations are affected by collision rates to and from the 
superlevels.  These rates are calculated by fitting to more complete 
population kinetic calculations involving hundreds of levels; these 
fits are valid only up to (electron) densities of 10$^{18}$ cm$^{-3}$.
Attempts to use densities greater than this will result in the 
code stopping with a message.

\section{The Energy Grid}

Many of the most important components of the XSTAR calculation require 
numerical quadratures over energy, and these are generally carried out 
using straight-forward trapezoid quadratures over a fixed grid of 
energies.  In addition, the computation is speeded by the use of 
a strictly logarithmic grid spacing in energy.  We use 9999 energies 
spaced logarithmically from 0.1 eV to 20keV.  This results in a 0.12$\%$
grid spacing, corresponding to, e.g. 8.6 eV at 7 keV.  This is the 
energy resolution of the code.

\section{The Ionizing Spectrum}

The ionizing spectrum has the obvious effect of creating ionization in 
the gas.  But it also can influence the heating and cooling via Compton 
scattering if the gas is highly ionized.  The standard ionizing spectrum 
options apply to all the energies in the grid.  Therefore if, for example,
an $\varepsilon^{-1}$ power law is chosen the temperature in Compton 
equilibrium will be kT=$(\varepsilon_{max}-\varepsilon_{min})
/(4 {\rm ln}(\varepsilon_{max}/\varepsilon_{min}))$, and 
power law indeces which are greater (or less) than 1 will be influenced
even more strongly by the choice of minimum (or maximum) energy.
It is likely that the choice we have made in designing the code 
will not be the choice which is physically appropriate for the 
situation of interest, so the user is encouraged to input the spectral
model from a file, with the appropriate cutoffs built in, 
in situations where power law spectra and Compton heating/cooling
are important.

\section{The use of critf}

This input parameter allows the user to control the size of the matrix solved when calculating 
level populations.  

In versions 2.1lxx and before this matrix had 
a maximum size of 2400, and an attempt to solve for more than this number of levels 
simultaneously would result in xstar stopping with a message `ipmat too large'.
This is most likely to occur in situations where more than 4 or 5 of the lower ion stages of 
iron meet the critf criterion; for problems with zero iron abundance critf can 
probably be specified as small as 1.e-15; when iron is non-zero the limit may be reached
if critf is less than 1.e-8.

With version 2.2 this is no longer true and small values of this parameter are recommended.

\section{Energy Budget}

In models where thermal equilibrium is imposed the total amount of energy 
absorbed from the incident radiation field should balance the total 
emitted energy in lines and continua.  This constraint is not 
automatically satisfied, since the algorithm for calculating heating and 
cooling rates is based on the assumption that each spatial zone is 
at most marginally optically thick.  As a check the total energy 
absorbed and emitted from the radiation field is printed at the end 
of the log file, along with the fractional error in energy conservation.
An error greater than a few percent indicates an inaccuracy 
in the model results which may significantly compromise emission line 
strengths, for example.  Such models should propbably be rerun with smaller 
values of emult.

\section{Column Density}

Xstar chooses spatial steps using a Courant condition (with 
limits) based on the local opacity.  It doesn't pay attention to 
where the cloud boundary is during this procedure, but rather checks 
after each step to see if it has gone too far.  If yes, it stops.
It does not go back and redo the step if it has gone 
significantly past the column density limit specified in the 
parameter file.  If this problem appears, you need to change 
the limits of the step size calculator to something smaller.
This is done using the parameter emult (normally hidden) in xstar.par.
The default is 0.5; values smaller by a factor 5 -- 10 
may slow execution somewhat but will solve the problem.

\section{Notes regarding equivalent widths of unresolved absorption lines in mtables}

The mtable results are qualitatively different between version 2.1h and 2.1j and later
for the depths of lines when the value of vturbi is small.
These differences are entirely due to the challenge of modeling absorption
in a binned spectrum.  Since before v2.1h the ionization and thermal
balance has changed very little.  

Taking O VIII L alpha as an example.  A cloud with colum 10$^{21.3}$,
log($\xi$)=1.8 and a $\gamma$=2 power law ionizing spectrum has a line center
optical depth of 1.6e3 for this line, and a temperature 10$^{5.13}$ K.  So the
thermal Doppler velocity is 11 km/s, or 2.4e-3 eV in energy units.  The
line wavelength is 18.9689 A, or 653.62 eV.  The nearest xstar bin
boundaries are at 653.1 and 654.0 eV, which are both many Doppler widths
away.  These versions of xstar both use a logarithmic energy grid evenly
spaced between 0.1 and 20000 eV, with E/DeltaE=722, for a total of 9999
energy bins (there are some extras tacked on at high energy). The version
in development has 10 times as many, but still will not resolve the lines
such as O VIII L alpha.

How to put an unresolved line on a fixed grid?  There are  3 obvious ways, 2
of them dumb: Version 2.1h and before simply calculated exp(-tau\_line) at
the nearest energy grid point, so O VIII L alpha was black in one bin.
This version also did not take into account the damping wings accurately.
Version 2.1j and later use an accurate Voigt profile, with damping wings,
and evaluate the profile function at the grind point.  So the bin at 654
eV in my example, being ~12 Doppler widths away, has an optical depth of
about 0.4.  And there was an error in the implementation of this in
versions 2.1j and 2.1k when turbulent broadening is included, but that is
now fixed.

The third way would be give the bin a depth which would give the correct 
equivalent width for this unresolved line, since neither of the previous
two will do that.  This is work, since it requires implementing a curve
of growth calculator which can handle damping, and arbitrary values for
hte line wavelength relative to the bin boundaries, and can smoothly go
over to something like what is in 2.1k when the line is resolved.  It 
is planned to put this in the next version of xstar.  

\subsection{Bug in version 2.1k}

The released version 2.1k has been found to have a bug in the Voigt profile calculation
of absorption line profiles.  This causes Infs in the opacity for lines with small 
damping parameters on some machines.  Xstar itself does not halt due to this error, 
and produces optical depths which are large in the core of the affected lines.
Xstar2xspec can halt with an arithmetic error caused by the inability of fitsio to 
read the large opacity.  This bug has been repaired in version 2.1kn and later.


